import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F

# Job parametrelerini al
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_PATH', 'OUTPUT_PATH'])

# Spark ve Glue context oluÅŸtur
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

print("ğŸš€ JSON to Parquet ETL Job baÅŸlatÄ±lÄ±yor...")
print(f"ğŸ“¥ Input Path: {args['INPUT_PATH']}")
print(f"ğŸ“¤ Output Path: {args['OUTPUT_PATH']}")

try:
    # JSON dosyasÄ±nÄ± oku
    print("ğŸ“Š JSON dosyasÄ± okunuyor...")
    df = spark.read.json(args['INPUT_PATH'])
    
    print(f"âœ… JSON dosyasÄ± okundu. KayÄ±t sayÄ±sÄ±: {df.count()}")
    
    # Veri tiplerini dÃ¼zelt
    print("ğŸ”„ Veri tipleri dÃ¼zeltiliyor...")
    df = df.withColumn("id", F.col("id").cast("int")) \
           .withColumn("age", F.col("age").cast("int")) \
           .withColumn("salary", F.col("salary").cast("double")) \
           .withColumn("hire_date", F.to_date(F.col("hire_date"), "yyyy-MM-dd"))
    
    # JSON'a Ã¶zel dÃ¶nÃ¼ÅŸÃ¼mler
    print("ğŸ”„ JSON dÃ¶nÃ¼ÅŸÃ¼mleri yapÄ±lÄ±yor...")
    df_transformed = df.withColumn(
        "experience_years",
        F.datediff(F.current_date(), F.col("hire_date")) / 365
    ).withColumn(
        "salary_per_experience",
        F.col("salary") / (F.datediff(F.current_date(), F.col("hire_date")) / 365 + 1)
    ).withColumn(
        "city_category",
        F.when(F.col("city").isin(["Ä°stanbul", "Ankara", "Ä°zmir"]), "BÃ¼yÃ¼k Åehir")
        .otherwise("DiÄŸer")
    ).withColumn(
        "processing_timestamp",
        F.current_timestamp()
    )
    
    # Parquet olarak kaydet
    print("ğŸ’¾ Parquet formatÄ±nda kaydediliyor...")
    df_transformed.write.mode("overwrite").parquet(args['OUTPUT_PATH'])
    
    print("âœ… JSON to Parquet dÃ¶nÃ¼ÅŸÃ¼mÃ¼ tamamlandÄ±!")
    print(f"ğŸ“ˆ Ä°ÅŸlenen kayÄ±t sayÄ±sÄ±: {df_transformed.count()}")
        
        # Ä°statistikler
    print("\nğŸ“Š Veri Ä°statistikleri:")
    df_transformed.groupBy("city_category").count().show()
    df_transformed.select(F.avg("experience_years").alias("avg_experience")).show()
        
    except Exception as e:
        print(f"âŒ ETL iÅŸlemi hatasÄ±: {str(e)}")
        raise e
    
    finally:
        job.commit()
    print("ğŸ‰ Job tamamlandÄ±!")
