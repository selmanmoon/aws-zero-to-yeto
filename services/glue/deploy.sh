#!/bin/bash

# AWS ZERO to YETO - Glue Deployment Script (Direct AWS CLI)
# Bu script gerÃ§ek Glue job'larÄ± ve ETL pipeline'Ä± oluÅŸturur

set -e  # Hata durumunda script'i durdur

# Renkli Ã§Ä±ktÄ± iÃ§in fonksiyonlar
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# AWS CLI kontrolÃ¼
check_aws_cli() {
    if ! command -v aws &> /dev/null; then
        print_error "AWS CLI kurulu deÄŸil. LÃ¼tfen Ã¶nce AWS CLI'yi kurun."
        exit 1
    fi
    
    # AWS kimlik bilgilerini kontrol et
    if ! aws sts get-caller-identity &> /dev/null; then
        print_error "AWS kimlik bilgileri yapÄ±landÄ±rÄ±lmamÄ±ÅŸ. 'aws configure' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
        exit 1
    fi
    
    print_success "AWS CLI ve kimlik bilgileri hazÄ±r"
}

# DeÄŸiÅŸkenler
PROJECT_NAME="aws-zero-to-yeto-glue"
REGION="eu-west-1"
TIMESTAMP=$(date +%s)
STACK_NAME="${PROJECT_NAME}-${TIMESTAMP}"
BUCKET_NAME="${PROJECT_NAME}-${TIMESTAMP}"
ROLE_NAME="${PROJECT_NAME}-role-${TIMESTAMP}"
DATABASE_NAME="aws_zero_to_yeto_db_${TIMESTAMP}"
JOB_NAME_CSV="csv-to-parquet-job-${TIMESTAMP}"
JOB_NAME_JSON="json-to-parquet-job-${TIMESTAMP}"

print_info "ðŸš€ Glue Deployment baÅŸlatÄ±lÄ±yor..."
print_info "Stack adÄ±: $STACK_NAME"
print_info "BÃ¶lge: $REGION"
print_info "S3 Bucket: $BUCKET_NAME"
print_info "IAM Role: $ROLE_NAME"
print_info "Database: $DATABASE_NAME"

# AWS CLI kontrolÃ¼
check_aws_cli

# KlasÃ¶r yapÄ±sÄ± oluÅŸtur
print_info "ðŸ“ KlasÃ¶r yapÄ±sÄ± oluÅŸturuluyor..."
mkdir -p examples/data
mkdir -p examples/python

# S3 bucket oluÅŸtur
print_info "ðŸª£ S3 bucket oluÅŸturuluyor..."
aws s3 mb s3://$BUCKET_NAME --region $REGION 2>/dev/null || print_info "Bucket zaten mevcut"

# IAM Role oluÅŸtur
print_info "ðŸ” IAM Role oluÅŸturuluyor..."
cat > trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "glue.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam create-role \
    --role-name $ROLE_NAME \
    --assume-role-policy-document file://trust-policy.json \
    --region $REGION

# IAM Policy'leri ekle
print_info "ðŸ“‹ IAM Policy'ler ekleniyor..."
aws iam attach-role-policy \
    --role-name $ROLE_NAME \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole \
    --region $REGION

# S3 eriÅŸimi iÃ§in custom policy
cat > s3-policy.json << EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::$BUCKET_NAME",
                "arn:aws:s3:::$BUCKET_NAME/*"
            ]
        }
    ]
}
EOF

aws iam put-role-policy \
    --role-name $ROLE_NAME \
    --policy-name S3AccessPolicy \
    --policy-document file://s3-policy.json \
    --region $REGION

# Role ARN'Ä± al
ROLE_ARN=$(aws iam get-role --role-name $ROLE_NAME --query 'Role.Arn' --output text --region $REGION)
print_success "IAM Role oluÅŸturuldu: $ROLE_ARN"

# Glue Database oluÅŸtur
print_info "ðŸ—„ï¸ Glue Database oluÅŸturuluyor..."
aws glue create-database \
    --database-input Name=$DATABASE_NAME,Description="AWS ZERO to YETO Glue Database" \
    --region $REGION

print_success "Glue Database oluÅŸturuldu: $DATABASE_NAME"

# Ã–rnek veri dosyalarÄ± oluÅŸtur
print_info "ðŸ“Š Ã–rnek veri dosyalarÄ± oluÅŸturuluyor..."

# CSV dosyasÄ±
cat > examples/data/sample_data.csv << 'EOF'
id,name,age,city,salary,department,hire_date
1,Ahmet YÄ±lmaz,28,Ä°stanbul,75000,IT,2020-01-15
2,AyÅŸe Kaya,32,Ankara,82000,HR,2019-03-22
3,Mehmet Demir,25,Ä°zmir,68000,Finance,2021-06-10
4,Fatma Åžahin,29,Bursa,71000,IT,2020-11-05
5,Ali Ã–zkan,35,Antalya,89000,Sales,2018-09-12
6,Zeynep Arslan,27,Ä°stanbul,73000,Marketing,2021-02-28
7,Murat KÄ±lÄ±Ã§,31,Ankara,85000,IT,2019-07-18
8,Elif Ã‡elik,26,Ä°zmir,69000,HR,2022-01-10
EOF

# JSON dosyasÄ±
cat > examples/data/sample_data.json << 'EOF'
[
  {"id": 1, "name": "Ahmet YÄ±lmaz", "age": 28, "city": "Ä°stanbul", "salary": 75000, "department": "IT", "hire_date": "2020-01-15"},
  {"id": 2, "name": "AyÅŸe Kaya", "age": 32, "city": "Ankara", "salary": 82000, "department": "HR", "hire_date": "2019-03-22"},
  {"id": 3, "name": "Mehmet Demir", "age": 25, "city": "Ä°zmir", "salary": 68000, "department": "Finance", "hire_date": "2021-06-10"},
  {"id": 4, "name": "Fatma Åžahin", "age": 29, "city": "Bursa", "salary": 71000, "department": "IT", "hire_date": "2020-11-05"},
  {"id": 5, "name": "Ali Ã–zkan", "age": 35, "city": "Antalya", "salary": 89000, "department": "Sales", "hire_date": "2018-09-12"},
  {"id": 6, "name": "Zeynep Arslan", "age": 27, "city": "Ä°stanbul", "salary": 73000, "department": "Marketing", "hire_date": "2021-02-28"},
  {"id": 7, "name": "Murat KÄ±lÄ±Ã§", "age": 31, "city": "Ankara", "salary": 85000, "department": "IT", "hire_date": "2019-07-18"},
  {"id": 8, "name": "Elif Ã‡elik", "age": 26, "city": "Ä°zmir", "salary": 69000, "department": "HR", "hire_date": "2022-01-10"}
]
EOF

# Veri dosyalarÄ±nÄ± S3'e yÃ¼kle
print_info "â¬†ï¸ Veri dosyalarÄ± S3'e yÃ¼kleniyor..."
aws s3 cp examples/data/sample_data.csv s3://$BUCKET_NAME/input/csv/sample_data.csv --region $REGION
aws s3 cp examples/data/sample_data.json s3://$BUCKET_NAME/input/json/sample_data.json --region $REGION

# CSV to Parquet ETL script oluÅŸtur
print_info "ðŸ“ CSV to Parquet ETL script oluÅŸturuluyor..."
cat > examples/python/csv_to_parquet.py << 'EOF'
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F

# Job parametrelerini al
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_PATH', 'OUTPUT_PATH'])

# Spark ve Glue context oluÅŸtur
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

print("ðŸš€ CSV to Parquet ETL Job baÅŸlatÄ±lÄ±yor...")
print(f"ðŸ“¥ Input Path: {args['INPUT_PATH']}")
print(f"ðŸ“¤ Output Path: {args['OUTPUT_PATH']}")

try:
    # CSV dosyasÄ±nÄ± oku
    print("ðŸ“Š CSV dosyasÄ± okunuyor...")
    df = spark.read.option("header", "true").csv(args['INPUT_PATH'])
    
    print(f"âœ… CSV dosyasÄ± okundu. KayÄ±t sayÄ±sÄ±: {df.count()}")
    
    # Veri tiplerini dÃ¼zelt
    print("ðŸ”„ Veri tipleri dÃ¼zeltiliyor...")
    df = df.withColumn("id", F.col("id").cast("int")) \
           .withColumn("age", F.col("age").cast("int")) \
           .withColumn("salary", F.col("salary").cast("double")) \
           .withColumn("hire_date", F.to_date(F.col("hire_date"), "yyyy-MM-dd"))
    
    # Veri dÃ¶nÃ¼ÅŸÃ¼mleri ekle
    print("ðŸ”„ Veri dÃ¶nÃ¼ÅŸÃ¼mleri yapÄ±lÄ±yor...")
    df_transformed = df.withColumn(
        "age_group",
        F.when(F.col("age") < 30, "GenÃ§")
        .when(F.col("age") < 40, "Orta YaÅŸ")
        .otherwise("Deneyimli")
    ).withColumn(
        "salary_category",
        F.when(F.col("salary") < 70000, "DÃ¼ÅŸÃ¼k")
        .when(F.col("salary") < 80000, "Orta")
        .otherwise("YÃ¼ksek")
    ).withColumn(
        "processing_date",
        F.current_date()
    )
    
    # Parquet olarak kaydet
    print("ðŸ’¾ Parquet formatÄ±nda kaydediliyor...")
    df_transformed.write.mode("overwrite").parquet(args['OUTPUT_PATH'])
    
    print("âœ… CSV to Parquet dÃ¶nÃ¼ÅŸÃ¼mÃ¼ tamamlandÄ±!")
    print(f"ðŸ“ˆ Ä°ÅŸlenen kayÄ±t sayÄ±sÄ±: {df_transformed.count()}")
    
    # Ä°statistikler
    print("\nðŸ“Š Veri Ä°statistikleri:")
    df_transformed.groupBy("department").count().show()
    df_transformed.groupBy("age_group").count().show()
        
    except Exception as e:
    print(f"âŒ ETL iÅŸlemi hatasÄ±: {str(e)}")
    raise e

finally:
    job.commit()
    print("ðŸŽ‰ Job tamamlandÄ±!")
EOF

# JSON to Parquet ETL script oluÅŸtur
print_info "ðŸ“ JSON to Parquet ETL script oluÅŸturuluyor..."
cat > examples/python/json_to_parquet.py << 'EOF'
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F

# Job parametrelerini al
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_PATH', 'OUTPUT_PATH'])

# Spark ve Glue context oluÅŸtur
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

print("ðŸš€ JSON to Parquet ETL Job baÅŸlatÄ±lÄ±yor...")
print(f"ðŸ“¥ Input Path: {args['INPUT_PATH']}")
print(f"ðŸ“¤ Output Path: {args['OUTPUT_PATH']}")

try:
    # JSON dosyasÄ±nÄ± oku
    print("ðŸ“Š JSON dosyasÄ± okunuyor...")
    df = spark.read.json(args['INPUT_PATH'])
    
    print(f"âœ… JSON dosyasÄ± okundu. KayÄ±t sayÄ±sÄ±: {df.count()}")
    
    # Veri tiplerini dÃ¼zelt
    print("ðŸ”„ Veri tipleri dÃ¼zeltiliyor...")
    df = df.withColumn("id", F.col("id").cast("int")) \
           .withColumn("age", F.col("age").cast("int")) \
           .withColumn("salary", F.col("salary").cast("double")) \
           .withColumn("hire_date", F.to_date(F.col("hire_date"), "yyyy-MM-dd"))
    
    # JSON'a Ã¶zel dÃ¶nÃ¼ÅŸÃ¼mler
    print("ðŸ”„ JSON dÃ¶nÃ¼ÅŸÃ¼mleri yapÄ±lÄ±yor...")
    df_transformed = df.withColumn(
        "experience_years",
        F.datediff(F.current_date(), F.col("hire_date")) / 365
    ).withColumn(
        "salary_per_experience",
        F.col("salary") / (F.datediff(F.current_date(), F.col("hire_date")) / 365 + 1)
    ).withColumn(
        "city_category",
        F.when(F.col("city").isin(["Ä°stanbul", "Ankara", "Ä°zmir"]), "BÃ¼yÃ¼k Åžehir")
        .otherwise("DiÄŸer")
    ).withColumn(
        "processing_timestamp",
        F.current_timestamp()
    )
    
    # Parquet olarak kaydet
    print("ðŸ’¾ Parquet formatÄ±nda kaydediliyor...")
    df_transformed.write.mode("overwrite").parquet(args['OUTPUT_PATH'])
    
    print("âœ… JSON to Parquet dÃ¶nÃ¼ÅŸÃ¼mÃ¼ tamamlandÄ±!")
    print(f"ðŸ“ˆ Ä°ÅŸlenen kayÄ±t sayÄ±sÄ±: {df_transformed.count()}")
        
        # Ä°statistikler
    print("\nðŸ“Š Veri Ä°statistikleri:")
    df_transformed.groupBy("city_category").count().show()
    df_transformed.select(F.avg("experience_years").alias("avg_experience")).show()
        
    except Exception as e:
        print(f"âŒ ETL iÅŸlemi hatasÄ±: {str(e)}")
        raise e
    
    finally:
        job.commit()
    print("ðŸŽ‰ Job tamamlandÄ±!")
EOF

# ETL script'leri S3'e yÃ¼kle
print_info "â¬†ï¸ ETL script'leri S3'e yÃ¼kleniyor..."
aws s3 cp examples/python/csv_to_parquet.py s3://$BUCKET_NAME/scripts/csv_to_parquet.py --region $REGION
aws s3 cp examples/python/json_to_parquet.py s3://$BUCKET_NAME/scripts/json_to_parquet.py --region $REGION

# Role'un propagate olmasÄ± iÃ§in bekle
print_info "â³ IAM Role propagation bekleniyor..."
sleep 10

# CSV to Parquet Glue Job oluÅŸtur
print_info "âš™ï¸ CSV to Parquet Glue Job oluÅŸturuluyor..."
aws glue create-job \
    --name $JOB_NAME_CSV \
    --role $ROLE_ARN \
    --command Name=glueetl,ScriptLocation=s3://$BUCKET_NAME/scripts/csv_to_parquet.py,PythonVersion=3 \
    --default-arguments '{
        "--job-bookmark-option": "job-bookmark-enable",
        "--enable-metrics": "",
        "--enable-continuous-cloudwatch-log": "true",
        "--enable-spark-ui": "true",
        "--spark-event-logs-path": "s3://'$BUCKET_NAME'/sparkHistoryLogs/"
    }' \
    --max-retries 0 \
    --timeout 60 \
    --glue-version "3.0" \
    --worker-type G.1X \
    --number-of-workers 2 \
    --region $REGION

print_success "CSV to Parquet Job oluÅŸturuldu: $JOB_NAME_CSV"

# JSON to Parquet Glue Job oluÅŸtur
print_info "âš™ï¸ JSON to Parquet Glue Job oluÅŸturuluyor..."
aws glue create-job \
    --name $JOB_NAME_JSON \
    --role $ROLE_ARN \
    --command Name=glueetl,ScriptLocation=s3://$BUCKET_NAME/scripts/json_to_parquet.py,PythonVersion=3 \
    --default-arguments '{
        "--job-bookmark-option": "job-bookmark-enable",
        "--enable-metrics": "",
        "--enable-continuous-cloudwatch-log": "true",
        "--enable-spark-ui": "true",
        "--spark-event-logs-path": "s3://'$BUCKET_NAME'/sparkHistoryLogs/"
    }' \
    --max-retries 0 \
    --timeout 60 \
    --glue-version "3.0" \
    --worker-type G.1X \
    --number-of-workers 2 \
    --region $REGION

print_success "JSON to Parquet Job oluÅŸturuldu: $JOB_NAME_JSON"

# Cleanup temp files
rm -f trust-policy.json s3-policy.json

# Deployment bilgilerini README'ye yaz
print_info "ðŸ“ Deployment bilgileri README'ye kaydediliyor..."

print_success "ðŸŽ‰ Glue deployment tamamlandÄ±!"
print_info "Stack: $STACK_NAME"
print_info "S3 Bucket: $BUCKET_NAME"
print_info "Database: $DATABASE_NAME"
print_info "CSV Job: $JOB_NAME_CSV"
print_info "JSON Job: $JOB_NAME_JSON"
print_info "Deployment bilgileri README'de mevcut"

print_warning "âš ï¸  Glue job'larÄ± Ã¼cretli kaynaklar. Test sonrasÄ± cleanup yapmayÄ± unutmayÄ±n!"

echo ""
print_info "ðŸ§ª Test komutlarÄ±:"
echo ""
echo "# CSV to Parquet Job'Ä± Ã§alÄ±ÅŸtÄ±r:"
echo "aws glue start-job-run \\"
echo "    --job-name $JOB_NAME_CSV \\"
echo "    --arguments '{"
echo "        \"--INPUT_PATH\": \"s3://$BUCKET_NAME/input/csv/\","
echo "        \"--OUTPUT_PATH\": \"s3://$BUCKET_NAME/output/csv-parquet/\""
echo "    }' \\"
echo "    --region $REGION"
echo ""
echo "# JSON to Parquet Job'Ä± Ã§alÄ±ÅŸtÄ±r:"
echo "aws glue start-job-run \\"
echo "    --job-name $JOB_NAME_JSON \\"
echo "    --arguments '{"
echo "        \"--INPUT_PATH\": \"s3://$BUCKET_NAME/input/json/\","
echo "        \"--OUTPUT_PATH\": \"s3://$BUCKET_NAME/output/json-parquet/\""
echo "    }' \\"
echo "    --region $REGION"
echo ""
echo "# S3 Ã§Ä±ktÄ±larÄ±nÄ± kontrol et:"
echo "aws s3 ls s3://$BUCKET_NAME/output/ --recursive --region $REGION"